[
["index.html", "Data Science Workshop Overview Requirements", " Data Science Workshop British Society for Proteomic Research Meeting 2018 Alistair Bailey July 02 2018 Overview This book covers: An introduction to R and RStudio An introduction to tidyverse and base R Importing and transforming proteomics data Visualisation of proteomics analysis The analysis is of an example data set of observations for 7702 proteins from cells in three control experiments and three treatment experiments. The observations are signal intensity measurements from the mass spectrometer. These intensities relate the concentration of protein observed in each experiment and under each condition. The analysis transforms the data to examine the effect of treatment on the cellular proteome and visualise the output using a volcano plot and a heatmap. Click here to download the csv file. Requirements An up to date version of R (R Core Team 2018) and RStudio (RStudio Team 2018). If you are new to R, then the first thing to know is that R is a programming language and RStudio is a program for working with R called an integrated development environment (IDE). You can use R without RStudio, but not the other way around. Further details in Chapter 1.1. Download R here and Download RStudio Desktop here. These materials were generated using R version 3.5.0. Once you’ve installed R and RStudio, you’ll also need a few R packages. Packages are collections of functions. Open RStudio and put the code below into the Console window and press Enter to install these three packages. install.packages(c(&quot;tidyverse&quot;,&quot;gplots&quot;,&quot;pheatmap&quot;)) References "],
["intro.html", "Chapter 1 Introduction 1.1 What are R and RStudio? 1.2 Why learn R, or any language ? 1.3 Finding your way around RStudio 1.4 Where am I? 1.5 R projects 1.6 Naming things 1.7 Seeking help", " Chapter 1 Introduction There are many resources for learning R on the web. Much of Chapters 1 2 and 3 derive from a Data Carpentry lesson using ecological data that I have previously reworked, which in turn takes a lot from Hadley Wickham’s R for Data Science. Follow the links to access those materials. Chapter 4 deals with some statistical transformations and visualisation methods in the context of proteomics data. Whilst finally in Chapter 5 there is some advice about how to build upon the materials covered here. In terms of philosophy: The primary motivation for using tools such as R is to get more done, in less time and with less pain. And the overall aim is to understand and communicate findings from our data. Figure 1.1: Data project workflow. As shown in Figure 1.1 of typical data analysis workflow, to acheive this aim we need to learn tools that enable us to perform the fundamental tasks of tasks of importing, tidying and often transforming the data. Transformation means for example, selecting a subset of the data to work with, or calculating the mean of a set of observations. We’ll cover that in Chapter 4. But first… 1.1 What are R and RStudio? “There are only two kinds of languages: the ones people complain about and the ones nobody uses” Bjarne Stroustrup R is a programming language that follows the philosophy laid down by it’s predecessor S. The philosophy being that users begin in an interactive environment where they don’t consciously think of themselves as programming. It was created in 1993, and documented in (Ihaka and Gentleman 1996). Reasons R has become popular include that it is both open source and cross platform, and that it has broad functionality, from the analysis of data and creating powerful graphical visualisations and web apps. Like all languages though it has limitations, for example the syntax is initially confusing. Take for example the word environment… 1.1.1 Environments An environment is where we bring our data to work with it. Here we work in a R envrionment, using the R language as a set of tools. RStudio is an integrated development environment, or IDE for R programming. It is regularly updated, and upgrading enables access to the latest features. The latest version can be downloaded here: http://www.rstudio.com/download 1.2 Why learn R, or any language ? We can write R code without saving it, but it’s generally more useful to write and save our code as a script. Working with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes. Learning R (or any programming language) and working with scripts forces you to have deeper understanding of what you are doing, facilitates your learning and comprehension of the methods you use: Writing and publishing code is important for reproducible resarch R has many thousands of packages covering many disciplines. R can work with many types of data. They is a large R community for development and support. Using R gives you control over your figures and reports. 1.3 Finding your way around RStudio Let’s begin by learning about RStudio, the Integrated Development Environment (IDE). We will use R Studio IDE to write code, navigate the files found on our computer, inspect the variables we are going to create, and visualize the plots we will generate. R Studio can also be used for other things (e.g., version control, developing packages, writing Shiny apps) that we don’t have time to cover during this workshop. R Studio is divided into “Panes”, see Figure 1.2. When you first open it, there are three panes,the console where you type commands, your environment/history (top-right), and your files/plots/packages/help/viewer (bottom-right). The enivronment shows all the R objects you have created or are using, such as data you have imported. The output pane can be used to view any plots you have created. Not opened at first start up is the fourth default pane: the script editor pane, but this will open as soon as we create/edit a R script (or many other document types). The script editor is where will be typing much of the time. Figure 1.2: The Rstudio Integrated Development Environment (IDE). The placement of these panes and their content can be customized (see menu, R Studio -&gt; Tools -&gt; Global Options -&gt; Pane Layout). One of the advantages of using R Studio is that all the information you need to write code is available ina single window. Additionally, with many shortcuts, auto-completion, and highlighting for the major file types you use while developing in R, R Studio will make typing easier and less error-prone. Time for a philosphical diversion… 1.3.1 What is real? At the start, we might consider our environment “real” - that is to say the objects we’ve created/loaded and are using are “real”. But it’s much better in the long run to consider our scripts as “real” - our scripts are where we write down the code that creates our objects that we’ll be using in our environment. As a script is a document, it is reproducible Or to put it another way: we can easily recreate an environment from our scripts, but not so easily create a script from an enivronment. To support this notion of thinking in terms of our scripts as real, we recommend turning off the preservation of workspaces between sessions by setting the Tools &gt; Global Options menu in R studio as shown in Figure 1.3: Figure 1.3: Don’t save your workspace, save your script instead. 1.4 Where am I? R studio tells you where you are in terms of directory address like so: (ref:working-dir) Figure 1.4: (ref:working-dir) If you are unfamiliar with how computers structure folders and files, then consider a tree with a root from which the trunk extends and branches divide. In the image above, the ~ symbol represents a contraction of the path from the root to the ‘home’ directory (in Windows this is ‘Documents’) and then the forward slashes are the branches. (Note: Windows uses backslashes, Unix type systems and R use forwardslashes). It is good practice to keep a set of related data, analyses, and text self-contained in a single folder, called the working directory. All of the scripts within this folder can then use relative paths to files that indicate where inside the project a file is located (as opposed to absolute paths, which point to where a file is on a specific computer). Working this way makes it a lot easier to move your project around on your computer and share it with others without worrying about whether or not the underlying scripts will still work. Figure 1.5: A typical directory structure 1.5 R projects RStudio also has a facility to keep all files associated with a particular analysis together called a project. Creating a project creates a working directory for you and also remembers its location (allowing you to quickly navigate to it) and optionally preserves custom settings and open files to make it easier to resume work after a break. Figure 1.6: Creating a R project Below, we will go through the steps for creating an “R Project”: Start R Studio (presentation of R Studio -below- should happen here) Under the File menu, click on New project, choose New directory, then Empty project Enter a name for this new folder (or “directory”, in computer science), and choose a convenient location for it. This will be your working directory for the rest of the day (e.g., ~/bspr-workshop) Click on “Create project” Under the Files tab on the right of the screen, click on New Folder and create a folder named data within your newly created working directory. (e.g., ~/bspr-workshopdata) Create a new R script (File &gt; New File &gt; R script) and save it in your working directory (e.g. bspr-workshop-script.R) 1.6 Naming things Jenny Bryan has three principles for naming things that are well worth remembering. When you names something, a file or an object, ideally it should be: Machine readable (no whitespace, punctuation, upper AND lowercase…) Human readable (makes sense in 6 months or 2 years time) Plays well with default ordering (numerical or date order) 1.7 Seeking help If you need help with a specific R function, let’s say barplot(), you can type: ?barplot If you can’t find what you are looking for, you can use the rdocumention.org website that searches through the help files across all packages available. A Google or internet search “R &lt;task&gt;” will often either send you to the appropriate package documentation or a helpful forum question that someone else already asked, such as Stack Overflow. 1.7.1 Asking for help As well as knowing where to ask, the key to get help from someone is for them to grasp your problem rapidly. You should make it as easy as possible to pinpoint where the issue might be. Try to use the correct words to describe your problem. For instance, a package is not the same thing as a library. Most people will understand what you meant, but others have really strong feelings about the difference in meaning. The key point is that it can make things confusing for people trying to help you. Be as precise as possible when describing your problem. If possible, try to reduce what doesn’t work to a simple reproducible example otherwise known as a reprex. For more information on how to write a reproducible example see this article. References "],
["tidyverse.html", "Chapter 2 Getting started in R and the tidyverse 2.1 The tidyverse and tidy data 2.2 Data visualisation 2.3 Workflow basics 2.4 Learning more R", " Chapter 2 Getting started in R and the tidyverse Functions are a way to automate common tasks and R comes with a set of functions called the base package. We will be using some base functions in Chapter 4, but to introduce the concept of using functions we’ll begin with the tidyverse. 2.1 The tidyverse and tidy data The tidyverse (Wickham 2017) is “an opinionated collection of R packages designed for data science” . Tidyverse packages contain functions that “share an underlying design philosophy, grammar, and data structures.” It’s this philiosophy that makes tidyverse functions and packages relatively easy to learn and use. Tidy data follows three principals for tabular data as proposed in the Tidy Data paper http://www.jstatsoft.org/v59/i10/paper : Every variable has its own column. Every observation has its own row. Each value has its own cell. If our table was proteomics data then, we might have a set of variables such as the peptide sequence, mass or length observed for a number of peptides. Therefore each peptide would have a row with columns for peptide sequence, mass and length with the value for each variable in separate cells, as seen in Figure 2.1. Figure 2.1: An example of tidy proteomics data Often much of the work in any data analysis is getting our data into a tidy form. We can’t do everything in the tidyverse, and everything we can do in the tidyverse can be done in what is called base R or other packages, but the motivation behind the tidyverse is to ease the pain of data manipulation. With this in mind, the two tasks we are most likely to want to do in data science are: Visualise our data Automate our processes. Taking our cue from R4DS let’s try an example. 2.2 Data visualisation The ggplot2 package implements the grammer of graphics, for describing and building graphs. The motivation here is twofold: To begin to grasp the grammar of graphics approach to creating plots. This will be our first example of automating a task using a function. To demonstrate how plotting is often the most useful thing we can do when trying to understand our data. We’ll use the mpg dataset that comes with the tidyverse to examine the question do cars with big engines use more fuel than cars with small engines? Try ?mpg to learn more about the data. Engine size in litres is in the displ column. Fuel efficiency on the highway in miles per gallon is given in the hwy column. To create a plot of engine size displ (x-axis) against fuel efficiency hwy (y-axis) we do the following: Use the ggplot() function to create an empty graph. Provide ggplot with a first input or argument of the data (here mpg). Then we follow the ggplot function with a + sign to indicate we are going to add more code, followed by a geom_point() function to add a layer of points mapping some aesthetics for the x and y axes. Mapping is always paired to aesthetics aes(). An aesthetic is a visual property of the objects in your plot, such a point size, shape or point colour. Therefore to plot engine size (x-axis) against fuel efficiency (y-axis) we use the following code: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) This plot shows a negative relationship between engine size and fuel efficiency. Now try extending this code to include to add a colour aesthetic to the the aes() function, let colour = class, class being the veichle type. This should create a plot with as before but with the points coloured according to the viechle type to expand our understanding. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) Now we can see that as we might expect, bigger cars such as SUVs tend to have bigger engines and are also less fuel efficient, but some smaller cars such as 2-seaters also have big engines and greater fuel efficiency. Hence we have a more nuanced view with this additional aesthetic. Check out the ggplot2 documentation for all the aesthetic possibilities (and Google for examples): http://ggplot2.tidyverse.org/reference/ So now we have re-usable code snippet for generating plots in R: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Concretely, in our first example &lt;DATA&gt; was mpg, the &lt;GEOM_FUNCTION&gt; was geom_point() and the arguments we supplies to map our aesthetics &lt;MAPPINGS&gt; were x = displ, y = hwy. As we can use this code for any tidy data set, hopefully you are beginning to see how a small amount of code can do a lot. 2.3 Workflow basics Let’s run through the basics of working in R to conclude this chapter. 2.3.1 Assigning objects Objects are just a way to store data inside the R environment. We create objects using the assignment operator &lt;-: mass_kg &lt;- 55 Read this as “mass_kg gets value 55” in your head. Using &lt;- can be annoying to type, so use RStudio’s keyboard short cut: Alt + - (the minus sign) to make life easier. Many people ask why we use this assignment operator when we can use = instead? Colin Fay had a Twitter thread on this subject, but the reason I favour most is that it provides clarity. The arrow points in the direction of the assigment (it is actually possible to assign in the other direction too) and it distinguishes between creating an object in the workspace and assigning a value inside a function. Object name style is a matter of choice, but must start with a letter and can only contain letters, numbers, _ and .. We recommend using descriptive names and using _ between words. Some special symbols cannot be used in variable names, so watch out for those. So here we’ve used the name to indicate its value represents a mass in kilograms. Look in your environment pane and you’ll see the mass_kg object containing the (data) value 55. We can inspect an object by typing it’s name: mass_kg ## [1] 55 What’s wrong here? mass_KG Error: object 'mass_KG' not found This error illustrates that typos matter, everything must be precise and mass_KG is not the same as mass_kg. mass_KG doesn’t exist, hence the error. 2.3.2 Function anatomy Functions in R are objects followed by parentheses, such as library(). Functions have the form: function_name(arg1 = val, arg2 = val2, ...) The use of arguements or inputs allows us to generalise. That is to say not just do something in a specific case, but in many cases. For example not just make a scatter plot for the mpg dataset, but for any dataset of observations that can be plotted pairwise. Let’s use seq() to create a sequence of numbers, and at the same time practice tab completion. Start typing se in the console and you should see a list of functions appear, add q to shorten the list, then use the up and down arrow to highlight the function of interest seq() and hit Tab to select. RStudio puts the cursor between the parentheses to prompt us to enter some arguments. Here we’ll use 1 as the start and 10 as the end: seq(1,10) ## [1] 1 2 3 4 5 6 7 8 9 10 If we left off a parentheses to close the function, then when we hit enter we’ll see a + indicating RStudio is expecting further code. We either add the missing part or press Escape to cancel the code. Let’s call a function and make an assignment at the same time. Here we’ll use the base R function seq() which takes three arguments: from, to and by. Read the following code as *“make an object called my_sequence that stores a sequence of numbers from 2 to 20 by intervals of 2*. my_sequence &lt;- seq(2,20,2) This time nothing was returned to the console, but we now have an object called my_sequence in our environment. Can you remember how to inspect it? If we want to subset elements of my_sequence we use square brackets []. For example element five would be subset by: my_sequence[5] ## [1] 10 Here the number five is the index of the vector, not the value of the fifth element. The value of the fifth element is 10. And returning multiple elements uses a colon :, like so my_sequence[5:8] ## [1] 10 12 14 16 2.3.3 Atomic vectors {#atomics]} We actually made an atomic vector already when we made my_sequence. We made a a one dimensional group of numbers, in a sequence from two to twenty. We’re not going to be working much with atomic vectors in this workshop, but to make you aware of how R stores data, atomic vector types are: Doubles: regular numbers, +ve or -ve and with or without decimal places. AKA numerics. Integers: whole numbers, specified with an upper-case L, e.g. int &lt;- 2L Characters: Strings of text Logicals: these store TRUEs and FALSEs which are useful for comparisons. Complex: this would be a vector of numbers with imaginary terms. Raw: these vectors store raw bytes of data. Let’s make a character vector and check the type: cards &lt;- c(&quot;ace&quot;, &quot;king&quot;, &quot;queen&quot;, &quot;jack&quot;, &quot;ten&quot;) cards ## [1] &quot;ace&quot; &quot;king&quot; &quot;queen&quot; &quot;jack&quot; &quot;ten&quot; typeof(cards) ## [1] &quot;character&quot; 2.3.4 Attributes An attribute is a piece of information you can attach to an object, such as names or dimensions. Attributes such as dimensions are added when we create an object, but others such as names can be added. Let’s look at the mpg data frame dimensions: # mpg has 234 rows (observations) and 11 columns (variables) dim(mpg) ## [1] 234 11 2.3.5 Factors Factors are Rs way of storing categorical information such as eye colour or car type. A factor is something that can only have certain values, and can be ordered (such as low,medium,high) or unordered such as types of fruit. Factors are useful as they code string variables such as “red” or “blue” to integer values e.g. 1 and 2, which can be used in statistical models and when plotting, but they are confusing as they look like strings. Factors look like strings, but behave like integers. Historically R converts strings to factors when we load and create data, but it’s often not what we want as a default. Fortunately, in the tidyverse strings are not treated as factors by default. 2.3.6 Lists Lists also group data into one dimensional sets of data. The difference being that list group objects instead of individual values, such as several atomic vectors. For example, let’s make a list containing a vector of numbers and a character vector list_1 &lt;- list(1:110,&quot;R&quot;) list_1 ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 ## [103] 103 104 105 106 107 108 109 110 ## ## [[2]] ## [1] &quot;R&quot; Note the double brackets to indicate the list elements, i.e. element one is the vector of numbers and element two is a vector of a single character. We won’t be working with lists in this workshop, but they are a flexible way to store data of different types in R. Accessing list elements uses double square brackets syntax, for example list_1[[1]] would return the first vector in our list. And to access the first element in the first vector would combine double and single square brackets like so: list_1[[1]][1]. Don’t worry if you find this confusing, everyone does when they first start with R. 2.3.7 Matrices and arrays Matrices store values in a two dimensional array, whilst arrays can have n dimensions. We won’t be using these either, but they are also valid R objects. 2.3.8 Data frames Data frames are two dimensional versions of lists, and this is form of storing data we are going to be using. In a data frame each atomic vector type becomes a column, and a data frame is formed by columns of vectors of the same length. Each column element must be of the same type, but the column types can vary. Figure 2.2 shows an example data frame we’ll refer to as saved as the object df consisting of three rows and three columns. Each column is a different atomic data type of the same length. Figure 2.2: An example data frame df. Packages in the tidyverse create a modified form of data frame called a tibble. You can read about tibbles here. One advantage of tibbles is that they don’t default to treating strings as factors. We deal with modifying data frames when we work with our example data set. Sub-setting data frames can also be done with square bracket syntax, but as we have both rows and columns, we need to provide index values for both row and column. For example df[1,2] means return the value of df row 1, column 2. This corresponds with the value A. We can also use the colon operator to choose several rows or columns, and by leaving the row or column blank we return all rows or all columns. # Subset rows 1 and 2 of column 1 df[1:2,1] # Subset all rows of column 3 df[,3] Again don’t worry too much about this for now, we won’t be doing to much of this in this lesson, but it’s important to be aware of the basic syntax. 2.4 Learning more R There are many places to start, but swirl can teach you interactively, and at your own pace in RStudio. Just follow the instructions via this link: http://swirlstats.com/students.html Hands-On Programming with R by Garrett Grolemund is another great resource for learning R. Plus all the tidyverse links. References "],
["import.html", "Chapter 3 Creating scripts and importing data 3.1 Some definitions 3.2 Using scripts 3.3 Running code 3.4 Creating a R script 3.5 Setting up our environment 3.6 Importing data 3.7 Exploring the data", " Chapter 3 Creating scripts and importing data Our analysis is of an example data set of observations for 7702 proteins from cells in three control experiments and three treatment experiments. The observations are signal intensity measurements from the mass spectrometer. These intensities relate the concentration of protein observed in each experiment and under each condition. We consider raw data as the data as we receive it. This doesn’t mean it hasn’t be processed in some way, it just means it hasn’t been processed by us. Generally speaking we don’t change the raw data file, what we do is import it and create an object in R which we then transform. So let’s understand how to import some data. 3.1 Some definitions Importing means getting data into our R environment by creating an object that we can then manipulate. The raw data file remains unchanged. Inspecting means looking at the dataset to understand what it contains. Tidying refers to getting data into a consistent format that makes it easy to use in later steps. 3.1.1 Rectangular data and flat formats Two further things to note: Here we are only considering rectangular data, the sort that comes in rows and columns such as in a spreadsheet. Lots of our data types exist, such as images, but can also be handled by R. As mentioned in 3.5.1 genomic data in particular has led to a project called Bioconductor for the development of analysis tools primarily in R, many of which deal with non-rectangular data, but this is beyond the scope here. Flat formats are files that only contain plain text, with each line representing a set of observations and the variables separated by delimiters such as tabs, commas or spaces. Therefore there aren’t multiple tables such as we’d get in an Excel file, or meta-data such as the colour highlighting of a cell in an Excel file. The advantages of flat files is that they can be opened and used by many different computing languages or programs. So unless there is a good reason not to use a flat format, and there are good reasons, they are the best way to store data in many situations. 3.2 Using scripts Using the console is useful, but as we build up a workflow, that is to say, writing code to: load packages load data explore the data and output some results Then it’s much more useful to contain this in a script: a document of our code. Why? When we write and save our code in scripts, we can re-use it, share it or edit it. But most importantly a script is a record. Cmd/Ctrl + Shift + N will open a new script file up and you should see something like Figure 3.1 with the script editor pane open: Figure 3.1: Rstudio with the script editor pane open. 3.3 Running code We can run a highlighted portion of code in your script if you click the Run button at the top of the scripts pane as shown in Figure 3.2. Figure 3.2: Scripts can be run by clicking the Source button. You can run the entire script by clicking the Source button. Or we can run chunks of code if we split our script into sections, see below. 3.4 Creating a R script We first need to create a script that will form the basis of our analysis. Go to the file menu and select New Files &gt; R script. This should open the script editor pane. Now let’s save the script, by going to File &gt; Save and we should find ourselves prompted to save the script in our Project Directory. Following the advice about naming things we can create a new R script called 01-bspr-workshop-july-2018. This name is machine readable (no spaces or special characters), human readable, and works well with default ordering by beginning with 01. 3.5 Setting up our environment At the head of our script it’s common to put a title, the name of the author and the date, and any other useful information. This is created as comments using the # at the start of each line. It’s then usual to follow this by code to load the packages we need into our our R environment using the library() function and providing the name of the package we wish to load. Packages are collections of R functions. Often we break the code up into regions by adding dashes (or equals symbols) to the comment line. This enables us to run chunks of the script separately from running the whole script when using our code. Here is a typical head for a script: # My workshop script # 7th July 2018 # Alistair Bailey # Load packages ---------------------------------------------------------------- library(plyr) library(tidyverse) library(gplots) library(pheatmap) 3.5.1 Bioconductor As an aside there are many proteomics specific R packages, these are generally found through Bioconductor which is a project that was initiated in 2001 to create tools for the analysis of high-throughput genomic data, but also includes other ’omics data tools (R. C. Gentleman et al. 2004,Huber et al. (2015)). Exploring Bioconductor is beyond our scope here, but well worth exploring for manipulation and analysis of raw data formats such as mzxml files. 3.6 Importing data Assuming our data is in a flat format, we can import it into our environment using the tidyverse readr package. If our data was an excel file, we can use the tidyverse readxl package to import the data, but it will remove any meta-data and each table in the excel file will become a separate R object as per tidy data principles. For the purposes of this workshop we have a csv (comma separated variable) file. If you haven’t done so already Click here to download the example data and save it to our project directory. Check the Files pane to see it’s there. We then import data and assign it to an object we’ll call data like so: # Import example data ---------------------------------------------------------- # Import the example data with read_csv from the readr package dat &lt;- readr::read_csv(&quot;data/070718-proteomics-example-data.csv&quot;) ## Parsed with column specification: ## cols( ## protein_accession = col_character(), ## protein_description = col_character(), ## control_1 = col_double(), ## control_2 = col_double(), ## control_3 = col_double(), ## treatment_1 = col_double(), ## treatment_2 = col_double(), ## treatment_3 = col_double() ## ) 3.7 Exploring the data 3.7.1 glimpse, head and str The first thing to do with any data set is to actually look at it. Here are four ways to have look at the data in the Console: calling the object directly, glimpse, head and str. We can just call the object and return it to the Console, which may or may not be useful depending on the size and type of object we call. 2 .glimpse is a tidyverse function that tries to show us as much data in a data.frame or tibble as possible, telling us the atomic types of data in the table, the number of observations and the number of variables, and importantly shows all the column variable names by transposing the table. head is a base function that shows us the 6 lines of a R object by default. str is a base function that show the structure of a R object, so it provides a lot of information, but is not so easy to read. The outputs for these four functions is shown below: # call object dat ## # A tibble: 7,702 x 8 ## protein_accession protein_description control_1 control_2 control_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 VATA_HUMAN_P38606 V-type proton ATPase c~ 0.811 0.858 1.04 ## 2 RL35A_HUMAN_P180~ 60S ribosomal protein ~ 0.367 0.385 0.409 ## 3 MYH10_HUMAN_P355~ Myosin-10 OS=Homo sapi~ 2.98 4.62 2.87 ## 4 RHOG_HUMAN_P84095 Rho-related GTP-bindin~ 0.142 0.224 0.128 ## 5 PSA1_HUMAN_P25786 Proteasome subunit alp~ 1.07 0.945 0.803 ## 6 PRDX5_HUMAN_P300~ Peroxiredoxin-5_ mitoc~ 0.566 0.540 0.488 ## 7 ACLY_HUMAN_P53396 ATP-citrate synthase O~ 5.00 4.22 5.03 ## 8 VDAC2_HUMAN_P458~ Voltage-dependent anio~ 1.35 1.33 1.14 ## 9 LRC47_HUMAN_Q8N1~ Leucine-rich repeat-co~ 0.927 0.770 1.17 ## 10 CH60_HUMAN_P10809 60 kDa heat shock prot~ 9.45 8.41 10.4 ## # ... with 7,692 more rows, and 3 more variables: treatment_1 &lt;dbl&gt;, ## # treatment_2 &lt;dbl&gt;, treatment_3 &lt;dbl&gt; # tidyverse glimpse function glimpse(dat) ## Observations: 7,702 ## Variables: 8 ## $ protein_accession &lt;chr&gt; &quot;VATA_HUMAN_P38606&quot;, &quot;RL35A_HUMAN_P18077&quot;,... ## $ protein_description &lt;chr&gt; &quot;V-type proton ATPase catalytic subunit A ... ## $ control_1 &lt;dbl&gt; 0.8114, 0.3672, 2.9815, 0.1424, 1.0748, 0.... ## $ control_2 &lt;dbl&gt; 0.8575, 0.3853, 4.6176, 0.2238, 0.9451, 0.... ## $ control_3 &lt;dbl&gt; 1.0381, 0.4091, 2.8709, 0.1281, 0.8032, 0.... ## $ treatment_1 &lt;dbl&gt; 0.6448, 0.4109, 7.1670, 0.1643, 0.7884, 0.... ## $ treatment_2 &lt;dbl&gt; 0.7190, 0.4634, 2.0052, 0.2466, 0.8798, 1.... ## $ treatment_3 &lt;dbl&gt; 0.4805, 0.3561, 0.8995, 0.1268, 0.7631, 0.... # head function head(dat) ## # A tibble: 6 x 8 ## protein_accession protein_description control_1 control_2 control_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 VATA_HUMAN_P38606 V-type proton ATPase ca~ 0.811 0.858 1.04 ## 2 RL35A_HUMAN_P180~ 60S ribosomal protein L~ 0.367 0.385 0.409 ## 3 MYH10_HUMAN_P355~ Myosin-10 OS=Homo sapie~ 2.98 4.62 2.87 ## 4 RHOG_HUMAN_P84095 Rho-related GTP-binding~ 0.142 0.224 0.128 ## 5 PSA1_HUMAN_P25786 Proteasome subunit alph~ 1.07 0.945 0.803 ## 6 PRDX5_HUMAN_P300~ Peroxiredoxin-5_ mitoch~ 0.566 0.540 0.488 ## # ... with 3 more variables: treatment_1 &lt;dbl&gt;, treatment_2 &lt;dbl&gt;, ## # treatment_3 &lt;dbl&gt; # str function str(dat) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 7702 obs. of 8 variables: ## $ protein_accession : chr &quot;VATA_HUMAN_P38606&quot; &quot;RL35A_HUMAN_P18077&quot; &quot;MYH10_HUMAN_P35580&quot; &quot;RHOG_HUMAN_P84095&quot; ... ## $ protein_description: chr &quot;V-type proton ATPase catalytic subunit A OS=Homo sapiens GN=ATP6V1A PE=1 SV=2&quot; &quot;60S ribosomal protein L35a OS=Homo sapiens GN=RPL35A PE=1 SV=2&quot; &quot;Myosin-10 OS=Homo sapiens GN=MYH10 PE=1 SV=3&quot; &quot;Rho-related GTP-binding protein RhoG OS=Homo sapiens GN=RHOG PE=1 SV=1&quot; ... ## $ control_1 : num 0.811 0.367 2.982 0.142 1.075 ... ## $ control_2 : num 0.858 0.385 4.618 0.224 0.945 ... ## $ control_3 : num 1.038 0.409 2.871 0.128 0.803 ... ## $ treatment_1 : num 0.645 0.411 7.167 0.164 0.788 ... ## $ treatment_2 : num 0.719 0.463 2.005 0.247 0.88 ... ## $ treatment_3 : num 0.48 0.356 0.899 0.127 0.763 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 8 ## .. ..$ protein_accession : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ protein_description: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ control_1 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ control_2 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ control_3 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ treatment_1 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ treatment_2 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ treatment_3 : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; To see the data in a spreadsheet fashion use View(dat), note the capital V and a new tab will open. This can also be launched from the Environment tab by clicking on dat. Although this provides us with some useful information, such as the number of observations and variables, to understand more plotting the data will be helpful as we’ll see in Section 4.3. 3.7.2 Summary statisitics Another useful way to quickly get a sense of the data is to use the summary function, which will return summary of the spread of the data and importantly if there are missing values. We can see immediately below that the experimental replicates have different distributions, and missing values that we need to deal with in Chapter 4. summary(dat) ## protein_accession protein_description control_1 control_2 ## Length:7702 Length:7702 Min. : 0.001 Min. : 0.000 ## Class :character Class :character 1st Qu.: 0.143 1st Qu.: 0.132 ## Mode :character Mode :character Median : 0.345 Median : 0.322 ## Mean : 0.933 Mean : 0.845 ## 3rd Qu.: 0.959 3rd Qu.: 0.845 ## Max. :31.944 Max. :31.697 ## NA&#39;s :4888 NA&#39;s :4828 ## control_3 treatment_1 treatment_2 treatment_3 ## Min. : 0.001 Min. : 0.000 Min. : 0.002 Min. : 0.002 ## 1st Qu.: 0.149 1st Qu.: 0.112 1st Qu.: 0.135 1st Qu.: 0.101 ## Median : 0.388 Median : 0.286 Median : 0.319 Median : 0.254 ## Mean : 0.977 Mean : 0.795 Mean : 0.856 Mean : 0.675 ## 3rd Qu.: 0.999 3rd Qu.: 0.780 3rd Qu.: 0.880 3rd Qu.: 0.682 ## Max. :31.320 Max. :41.686 Max. :28.234 Max. :21.428 ## NA&#39;s :5087 NA&#39;s :4739 NA&#39;s :4902 NA&#39;s :5074 References "],
["transform.html", "Chapter 4 Transformation and visualisation 4.1 Fold change and log-fold change 4.2 Dealing with missing values 4.3 Data normalization 4.4 Visualising data 4.5 Creating a volcano plot 4.6 Creating a heatmap plot", " Chapter 4 Transformation and visualisation Having imported our data set of observations for 7702 proteins from cells in three control experiments and three treatment experiments. Remember, the observations are signal intensity measurements from the mass spectrometer, and these intensities relate to the amount of protein in each experiment and under each condition. Next we will transform the data to examine the effect of the treatment on the cellular proteome and visualise the output using a volcano plot and a heatmap. Concretely, the hypothesis we are testing is that treatment changes the concentration of protein we observe. 4.1 Fold change and log-fold change Fold changes are ratios, the ratio of say protein expression before and after treatment, where a value larger than 1 for a protein implies that protein expression was greater after the treatment. In life sciences, fold change is often reported as log-fold change. Why is that? There are at least two reasons which can be shown by plotting. One is that ratios are not symmetrical around 1, so it’s difficult to observe both changes in the forwards and backwards direcion i.e. proteins where expression went up and proteins where expression went down due to treatment. When we transform ratios on a log scale, the scale becomes symmetric around 0 and thus we can now observe the distribution of ratios in terms of positive, negative or no change. Figure 4.1: Ratios are not symmetric around one, logratios are symmetric around zero. A second reason is that transforming values onto a log scale changes where the numbers actually occur when plotted on that scale. If we consider the log scale to represent magnitudes, then we can more easily see changes of small and large magnitudes when we plot the data. For example, a fold change of 32 times can be either a ratio 1/32 or 32/1. As shown in Figure 4.2, 1/32 is much closer to 1 than 32/1, but transformed to a log scale we see that in terms of magnitude of difference it is the same as 32/1. Often the log transformation is to a base of 2 as each increment of 1 represents a doubling, but sometimes a base of 10 is used, for example for p-values. Figure 4.2: Transformation of scales using log transformation. 4.2 Dealing with missing values Unless we’re really lucky, it’s unlikely that we’ll get observations for the same numbers of proteins in all replicated experiments. This means there will be missing values for some proteins when looking at all the experiments together. This then raises the question of what to do about the missing values? We have two choices: Only analyse the proteins that we have observations for in all experiments. Impute values for the missing values from the existing observations. There are pros and cons to either approach. Here for simplicity we’ll use only the proteins for which we have observations in all assays. We can drop the proteins with missing values by piping our data set to the drop_na() function from the tidyr package like so. We assign this to a new object called dat_tidy. # Remove the missing values dat_tidy &lt;- dat %&gt;% drop_na() # Nunber of proteins in original data dat %&gt;% summarise(Number_of_proteins = n()) ## # A tibble: 1 x 1 ## Number_of_proteins ## &lt;int&gt; ## 1 7702 # Nunber of proteins without missing values dat_tidy %&gt;% summarise(Number_of_proteins = n()) ## # A tibble: 1 x 1 ## Number_of_proteins ## &lt;int&gt; ## 1 1145 This shrinks the dataset from 7,702 proteins to 1,145 proteins, so we can see why imputing the missing values might be more atrractive. One approach you might like to try is to impute the data by replacing the missing values with the mean observation for each protein under each condition. 4.3 Data normalization To perform statistical inference, for example whether treatment increases or decreases protein abundance, we need to account for the variation that occurs from run to run on our spectrometers and each give rise to a different distribution. This is as opposed to variation arising from treatment versus control which we are interested in understanding. Hence normalization seeks to reduce the run-to-run sources of variation. A method of normalization introduced for DNA microarray analysis is quantile normalization (B. M. Bolstad et al. 2003). If we consider our proteomics data as a distribution of values, one value for the concentration of each protein in our experiment that together form a distribution. Figure 4.3 shows the distribution of protein concentrations observed for the three control and three treatment assays. As we can see the distributions are different for each assay. Figure 4.3: Protein data for six assays plotted as a distributions. A quantile represents a region of distribution, for example the 0.95 quantile is the value such that 95% of the data lies below it. To normalize two or more distributions with each other without recourse to a reference distribution we: Rank the value in each experiment (represented in the columns) from lowest to highest. In other words identify the quantiles for each protein in each experiment. Sort each experiment (the columns) from lowest to highest value. Calculate the mean across the rows for the sorted values. Then substitute these mean values back according to rank for each experiment to restore the original order. This results in the highest ranking observation in each experiment becoming the mean of the highest observations across all experiments, the second ranking observation in each experiment becoming the mean of the second highest observations across all experiments. Therefore the distributions for each each experiment are now the same. Dave Tang’s Blog:Quantile Normalisation in R has more details on this approach. Figure 4.4: Quantile Normalisation from Rafael Irizarry’s tweet. These result of quantile normalization is that our distributions become statisitcally identitical, which we can see by plotting the densities of the normalized data. As shown in Figure 4.5 the distributions all overlay. # Quantile normalisation : the aim is to give different distributions the # same statistical properties quantile_normalisation &lt;- function(df){ # Find rank of values in each column df_rank &lt;- map_df(df,rank,ties.method=&quot;average&quot;) # Sort observations in each column from lowest to highest df_sorted &lt;- map_df(df,sort) # Find row mean on sorted columns df_mean &lt;- rowMeans(df_sorted) # Function for substiting mean values according to rank index_to_mean &lt;- function(my_index, my_mean){ return(my_mean[my_index]) } # Replace value in each column with mean according to rank df_final &lt;- map_df(df_rank,index_to_mean, my_mean=df_mean) return(df_final) } dat_norm &lt;- dat_tidy %&gt;% select(-c(1:2)) %&gt;% quantile_normalisation() %&gt;% bind_cols(dat_tidy[,1:2],.) Figure 4.5: Comparison of the protein distributions before normalization (left) and after quantile normalization (right). 4.3.1 T-test Having removed missing values and normalised the data, we can consider our hypothesis: treatement changes the amount of protein we observe in the cells. Things we need to consider in performing our t-test: We are perfoming a test for each protein for between three control and treatment experiments and assume unequal variances between the control and treatment for each protein. A look at a single protein supports this assumption: # # Variance of first control protein observations # var(c(dat_norm$control_1[1], dat_norm$control_2[1], dat_norm$control_3[1])) # # Variance of first treatment protein observations # var(c(dat_norm$treatment_1[1], dat_norm$treatment_2[1], dat_norm$treatment_3[1])) # # treatment &lt;- c(dat_norm$treatment_1[1], # dat_norm$treatment_2[1], # dat_norm$treatment_3[1]) # control &lt;- c(dat_norm$control_1[1], # dat_norm$control_2[1], # dat_norm$control_3[1]) # # t.test(treatment,control)$p.value Hence we will perform a Welch’s t-test for unequal variances. We don’t know whether the effect of the treatment is to increase or decrease the concentration of the protein, hence we will perform a two-sided t-test. The observations for the proteins are for proteins of the same type but from independent experiments, rather than observations of the same individuals before and after treatment. Hence we test the observations as unpaired samples. Use t.test to perform Welch Two Sample t-test on untransformed data. This outputs the p-values we need for each protein. I couldn’t figure out a tidy way to do this, so I am using the base R apply function here. What’s going on in the code below is the following, # T-test function for multiple experiments t_test &lt;- function(dt,grp1,grp2){ # Subset control group and convert to numeric x &lt;- dt[grp1] %&gt;% unlist %&gt;% as.numeric() # Subset treatment group and convert to numeric y &lt;- dt[grp2] %&gt;% unlist %&gt;% as.numeric() # Perform t-test using the mean of x and y result &lt;- t.test(x, y) # Extract p-values from the results p_vals &lt;- tibble(p_val = result$p.value) # Return p-values return(p_vals) } # Apply t-test function to data using plyr adply # .margins = 1, slice by rows, .fun = t_test plus t_test arguements dat_pvals &lt;- plyr::adply(dat_norm,.margins = 1, .fun = t_test, grp1 = c(3:5), grp2 = c(6:8)) Function p-val p-val 0.0927 0.0927 # Plot histogram dat_pvals %&gt;% ggplot(aes(p_val)) + geom_histogram(binwidth = 0.05, boundary = 0.5, fill = &quot;darkblue&quot;, colour = &quot;white&quot;) + xlab(&quot;p-value&quot;) + ylab(&quot;Frequency&quot;) + theme_minimal() Perform log transformation of the observations for each protein. # Select columns and log data exp_log &lt;- dat_pvals %&gt;% select(-c(protein_accession,protein_description,p_val)) %&gt;% log2() %&gt;% as.tibble() # Bind columns to create transformed data frame dat_log &lt;- bind_cols(dat_pvals[,c(1,2,9)], exp_log) %&gt;% select(protein_accession, protein_description, control_1, control_2,control_3, treatment_1,treatment_2,treatment_3, p_val) Calculate the mean observation for each protein under each condition. # Group mean function for multiple experiments grp_mean &lt;- function(dt,grp1,grp2){ # Subset control group and convert to numeric x &lt;- dt[grp1] %&gt;% unlist %&gt;% as.numeric() # Subset treatment group and convert to numeric y &lt;- dt[grp2] %&gt;% unlist %&gt;% as.numeric() # Calculate mean for each group mean_x &lt;- mean(x) mean_y &lt;- mean(y) # Make data frame of means mean_xy &lt;- tibble(mean_grp1 = mean_x, mean_grp2 = mean_y) # Return p-values return(mean_xy) } dat_fc &lt;- plyr::adply(dat_log,.margins = 1, .fun = grp_mean, grp1 = c(3:5), grp2 = c(6:8)) %&gt;% # Calculate fold change as difference between mean control and mean treatment mutate(log_fc = mean_grp1 - mean_grp2) # Final transformed data dat_tf &lt;- dat_fc %&gt;% select(protein_accession, protein_description, log_fc, p_val) %&gt;% mutate(log_pval = -1*log10(p_val)) %&gt;% select(-p_val) knitr::kable(dat_tf %&gt;% head(.,5), booktabs=TRUE) protein_accession protein_description log_fc log_pval VATA_HUMAN_P38606 V-type proton ATPase catalytic subunit A OS=Homo sapiens GN=ATP6V1A PE=1 SV=2 0.3687886 1.0327506 RL35A_HUMAN_P18077 60S ribosomal protein L35a OS=Homo sapiens GN=RPL35A PE=1 SV=2 -0.2505780 1.1400318 MYH10_HUMAN_P35580 Myosin-10 OS=Homo sapiens GN=MYH10 PE=1 SV=3 0.3838733 0.0056494 RHOG_HUMAN_P84095 Rho-related GTP-binding protein RhoG OS=Homo sapiens GN=RHOG PE=1 SV=1 -0.3417452 0.3775483 PSA1_HUMAN_P25786 Proteasome subunit alpha type-1 OS=Homo sapiens GN=PSMA1 PE=1 SV=1 0.0371316 0.0920101 The log fold change is then the difference between condition 1 and condition 2. # Plot a histogram to look at the distribution. dat_tf %&gt;% ggplot(aes(log_fc)) + geom_histogram(binwidth = 0.5, boundary = 0.5, fill = &quot;darkblue&quot;, colour = &quot;white&quot;) + xlab(&quot;log2 fold change&quot;) + ylab(&quot;Frequency&quot;) + theme_minimal() 4.4 Visualising data Based on empircal research, there are some general rules on visulisations that are worth bearing in mind: Plot 4.5 Creating a volcano plot A volcano plot is a plot of the log fold change in the observation between two conditions on the x-axis, for example the protein expression between treatment and control conditions. On the y-axis is the corresponding p-value for each observation, representing the likelihood that an observed change is due to the different conditions rather than arising from a natural variation in the fold change that might be observed if we performed many replications of the experiment. The aim of a volcano plot is to enable the viewer to quickly see the effect (if any) of an experiment with two conditions on many species (i.e. proteins) in terms of both an increase and decrease of the observed value. Like all plots it has it’s good and bad points, namely it’s good that we can visualise a lot of complex information in one plot. However this is also it’s main weakness, it’s rather complicated to understand in one glance. However, volcano plots are widely used in the literature, so there may be an amount of social proof giving rise to their popularity as much as their utility. dat_tf %&gt;% ggplot(aes(log_fc,log_pval)) + geom_point() However it would be much more useful with some extra formatting… dat_tf %&gt;% # Add a threhold for significant observations mutate(threshold = if_else(log_fc &gt;= 2 &amp; log_pval &gt;= 1.5 | log_fc &lt;= -2 &amp; log_pval &gt;= 1.5,&quot;A&quot;, &quot;B&quot;)) %&gt;% # Plot with points coloured according to the threshold ggplot(aes(log_fc,log_pval, colour = threshold)) + geom_point(alpha = 0.5) + # Alpha sets the transparency of the points # Add dotted lines to indicate the threshold, semi-transparent geom_hline(yintercept = 1.5, linetype = 2, alpha = 0.5) + geom_vline(xintercept = 2, linetype = 2, alpha = 0.5) + geom_vline(xintercept = -2, linetype = 2, alpha = 0.5) + # Set the colour of the points scale_colour_manual(values = c(&quot;A&quot;= &quot;red&quot;, &quot;B&quot;= &quot;black&quot;)) + xlab(&quot;log2 fold change&quot;) + ylab(&quot;-log10 p-value&quot;) + # Relabel the axes theme_minimal() + # Set the theme theme(legend.position=&quot;none&quot;) # Hide the legend But which proteins are the significant observations? dat_tf %&gt;% # Add a threhold for significant observations mutate(threshold = if_else(log_fc &gt;= 2 &amp; log_pval &gt;= 1.5 | log_fc &lt;= -2 &amp; log_pval &gt;= 1.5,&quot;A&quot;, &quot;B&quot;), prot_id = str_extract(protein_accession,&quot;.{6}$&quot;)) %&gt;% # Get last six characters # Filter observations above the threshold filter(threshold == &quot;A&quot;) ## protein_accession ## 1 AKA12_HUMAN_Q02952 ## 2 GFPT2_HUMAN_O94808 ## 3 H7BYV1_HUMAN_H7BYV1 ## 4 ITAV_HUMAN_P06756 ## 5 CHD5_HUMAN_Q8TDI0 ## protein_description ## 1 A-kinase anchor protein 12 OS=Homo sapiens GN=AKAP12 PE=1 SV=4 ## 2 Glutamine--fructose-6-phosphate aminotransferase [isomerizing] 2 OS=Homo sapiens GN=GFPT2 PE=1 SV=3 ## 3 Interferon-induced transmembrane protein 2 (Fragment) OS=Homo sapiens GN=IFITM2 PE=4 SV=1 ## 4 Integrin alpha-V OS=Homo sapiens GN=ITGAV PE=1 SV=2 ## 5 Chromodomain-helicase-DNA-binding protein 5 OS=Homo sapiens GN=CHD5 PE=1 SV=1 ## log_fc log_pval threshold prot_id ## 1 -2.286581 1.516250 A Q02952 ## 2 -3.085366 2.450191 A O94808 ## 3 2.052333 2.457640 A H7BYV1 ## 4 -2.219137 2.617353 A P06756 ## 5 5.035230 1.595666 A Q8TDI0 4.6 Creating a heatmap plot # Keep the same p-val cut-off, but relax the log_fc to 1 which represents a # doubling dat_mut &lt;- dat_norm %&gt;% mutate(log_pval = dat_tf$log_pval, log_fc = dat_tf$log_fc) %&gt;% filter(log_pval &gt;= 1.5 &amp; (log_fc &gt;= 1 | log_fc &lt;= -1)) %&gt;% select(-c(2,9:10)) dat_sel &lt;- as.matrix.data.frame(dat_mut[,2:7]) %&gt;% log2() row.names(dat_sel) &lt;- dat_mut$protein_accession dat.tn &lt;- scale(t(dat_sel)) %&gt;% t() #dat.tn &lt;- t(dat.n) #dat.tn &lt;- dat_sel #gplots::heatmap.2(dat.tn, scale = &#39;row&#39;,trace=&quot;none&quot;) pheatmap(dat.tn,cutree_rows = 2, cutree_cols = 2, fontsize_row = 6) cal_z_score &lt;- function(x){ (x - mean(x)) / sd(x) } data_subset_norm &lt;- t(apply(dat_mut[,2:7], 1, cal_z_score)) data_subset_norm %&gt;% magrittr::set_colnames(c(&quot;Ctl 1&quot;, &quot;Ctl 2&quot;, &quot;Ctl 3&quot;, &quot;Trt 1&quot;, &quot;Trt 2&quot;, &quot;Trt 3&quot;)) %&gt;% magrittr::set_rownames(dat_mut$protein_accession) %&gt;% pheatmap(., fontsize = 6, cutree_rows = 2, cutree_cols = 2) d1 &lt;- dat.tn %&gt;% t() %&gt;% dist(.,method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE) d2 &lt;- dat.tn %&gt;% dist(.,method = &quot;euclidean&quot;, diag = FALSE, upper = FALSE) # Clustering distance between experiments using Ward linkage c1 &lt;- hclust(d1, method = &quot;ward.D2&quot;, members = NULL) # Clustering distance between proteins using Ward linkage c2 &lt;- hclust(d2, method = &quot;ward.D2&quot;, members = NULL) # Check clustering by plotting dendrograms par(mfrow=c(2,1),cex=0.5) # Make 2 rows, 1 col plot frame and shrink labels plot(c1); plot(c2) # Plot both cluster dendrograms # Set colours for heatmap, 25 increments my_palette &lt;- colorRampPalette(c(&quot;blue&quot;,&quot;white&quot;,&quot;red&quot;))(n = 25) # Plot heatmap with heatmap.2 par(cex.main=0.75) # Shrink title fonts on plot dat.tn %&gt;% magrittr::set_colnames(c(&quot;Ctl 1&quot;, &quot;Ctl 2&quot;, &quot;Ctl 3&quot;, &quot;Trt 1&quot;, &quot;Trt 2&quot;, &quot;Trt 3&quot;)) %&gt;% magrittr::set_rownames(dat_mut$protein_accession) %&gt;% gplots::heatmap.2(., # Tidy, normalised data Colv=as.dendrogram(c1), # Experiments clusters in cols Rowv=as.dendrogram(c2), # Protein clusters in rows density.info=&quot;histogram&quot;, # Plot histogram of data and colour key trace=&quot;none&quot;, # Turn of trace lines from heat map col = my_palette, # Use my colour scheme cexRow=0.3,cexCol=0.75) # Amend row and column label fonts References "],
["going-further.html", "Chapter 5 Going further 5.1 Learning dplyr verbs 5.2 Getting help and joining the R community 5.3 Communication: creating reports, presentations and websites", " Chapter 5 Going further 5.1 Learning dplyr verbs 5.2 Getting help and joining the R community 5.3 Communication: creating reports, presentations and websites "],
["references.html", "References", " References "]
]
